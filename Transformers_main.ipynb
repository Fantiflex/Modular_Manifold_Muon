{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNSPKtx7UQDeT+upe5E6R9Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fantiflex/Modular_Manifold_Muon/blob/main/Transformers_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI6VY9F979hz",
        "outputId": "825f2edc-d008-4a71-e093-1e7853a756ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training VIT with: adam\n",
            "Epochs: 20 --- LR: 0.001 --- WD: 0.0\n",
            "Epoch 1, Loss: 1.9035, Time: 16.2980 seconds\n",
            "Epoch 2, Loss: 1.5903, Time: 15.5355 seconds\n",
            "Epoch 3, Loss: 1.4021, Time: 15.7451 seconds\n",
            "Epoch 4, Loss: 1.2736, Time: 15.7018 seconds\n",
            "Epoch 5, Loss: 1.1680, Time: 16.0391 seconds\n",
            "Epoch 6, Loss: 1.0991, Time: 16.0083 seconds\n",
            "Epoch 7, Loss: 1.0356, Time: 15.6703 seconds\n",
            "Epoch 8, Loss: 0.9829, Time: 15.5816 seconds\n",
            "Epoch 9, Loss: 0.9432, Time: 15.3213 seconds\n",
            "Epoch 10, Loss: 0.9123, Time: 15.5982 seconds\n",
            "Epoch 11, Loss: 0.8782, Time: 15.7600 seconds\n",
            "Epoch 12, Loss: 0.8391, Time: 15.2527 seconds\n",
            "Epoch 13, Loss: 0.8041, Time: 15.4981 seconds\n",
            "Epoch 14, Loss: 0.7828, Time: 15.5278 seconds\n",
            "Epoch 15, Loss: 0.7516, Time: 16.4243 seconds\n",
            "Epoch 16, Loss: 0.7305, Time: 16.0905 seconds\n",
            "Epoch 17, Loss: 0.7134, Time: 15.9614 seconds\n",
            "Epoch 18, Loss: 0.6938, Time: 15.4912 seconds\n",
            "Epoch 19, Loss: 0.6761, Time: 15.4057 seconds\n",
            "Epoch 20, Loss: 0.6609, Time: 16.2186 seconds\n",
            "Accuracy of the network on the 10000 test images: 69.2 %\n",
            "Accuracy of the network on the 50000 train images: 78.678 %\n",
            "Saving results to results/model-vit-update-adam-lr-0.001-wd-0.0-seed-42.pkl\n",
            "Results saved to results/model-vit-update-adam-lr-0.001-wd-0.0-seed-42.pkl\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# IMPORTS MUON (à adapter à ton projet)\n",
        "# -------------------------------------------------------------------\n",
        "# from ton_module_muon import manifold_muon, manifold_muon_general, hyperspherical_descent, ManifoldLBFGS\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# DATA\n",
        "# -------------------------------------------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        (0.49139968, 0.48215827, 0.44653124),\n",
        "        (0.24703233, 0.24348505, 0.26158768),\n",
        "    ),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, transform=transform, download=True\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, transform=transform, download=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# MODELS\n",
        "# -------------------------------------------------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_dim=512, num_classes=10):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, hidden_dim, bias=False)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        self.fc3 = nn.Linear(hidden_dim, num_classes, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 32 * 3)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN simple pour CIFAR-10.\n",
        "    On peut passer les convs et le fc sous Muon / L-BFGS (aplaties).\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(x)              # [B, 256, 1, 1]\n",
        "        x = x.view(x.size(0), -1)     # [B, 256]\n",
        "        x = self.fc(x)                # [B, 10]\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Petit Vision Transformer pour CIFAR-10.\n",
        "\n",
        "    - Image 32x32 -> patchs 4x4 -> 8x8=64 patchs.\n",
        "    - Embedding de patchs (Linear 48 -> embed_dim).\n",
        "    - Positional embedding appris.\n",
        "    - 2 couches TransformerEncoder (d_model=128, 4 heads).\n",
        "    - Average pooling sur les tokens -> Linear vers 10 classes.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        in_chans=3,\n",
        "        num_classes=10,\n",
        "        embed_dim=128,\n",
        "        depth=2,\n",
        "        num_heads=4,\n",
        "        mlp_ratio=4.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        patch_dim = in_chans * patch_size * patch_size\n",
        "\n",
        "        # Matrice 2D -> parfait pour Muon/L-BFGS\n",
        "        self.patch_embed = nn.Linear(patch_dim, embed_dim, bias=False)\n",
        "\n",
        "        # Embedding de position\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
        "            batch_first=True,\n",
        "            activation=\"gelu\",\n",
        "            norm_first=True,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        assert H == self.img_size and W == self.img_size\n",
        "\n",
        "        p = self.patch_size\n",
        "        x = x.reshape(B, C, H // p, p, W // p, p)\n",
        "        x = x.permute(0, 2, 4, 1, 3, 5).contiguous()\n",
        "        x = x.view(B, -1, C * p * p)        # [B, num_patches, patch_dim]\n",
        "\n",
        "        x = self.patch_embed(x)             # [B, num_patches, embed_dim]\n",
        "        x = x + self.pos_embed              # [B, num_patches, embed_dim]\n",
        "\n",
        "        x = self.encoder(x)                 # [B, num_patches, embed_dim]\n",
        "        x = x.mean(dim=1)                   # [B, embed_dim]\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.head(x)                    # [B, 10]\n",
        "        return x\n",
        "\n",
        "\n",
        "def build_model(model_type: str) -> nn.Module:\n",
        "    model_type = model_type.lower()\n",
        "    if model_type == \"mlp\":\n",
        "        return MLP()\n",
        "    elif model_type == \"cnn\":\n",
        "        return SimpleCNN()\n",
        "    elif model_type == \"vit\":\n",
        "        return SimpleViT()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# GLOBAL STATE FOR L-BFGS\n",
        "# -------------------------------------------------------------------\n",
        "OPTS = {}  # map: param -> ManifoldLBFGS instance\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# TRAIN\n",
        "# -------------------------------------------------------------------\n",
        "def train(epochs, initial_lr, update, wd, model_type):\n",
        "    global OPTS\n",
        "    OPTS = {}  # on reset pour chaque run\n",
        "\n",
        "    model = build_model(model_type).cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Cas AdamW vs \"update manifold\"\n",
        "    if update == AdamW:\n",
        "        optimizer = AdamW(model.parameters(), lr=initial_lr, weight_decay=wd)\n",
        "    else:\n",
        "        optimizer = None  # Muon / hyperspherical / L-BFGS\n",
        "\n",
        "    # Est-ce qu'on est dans le cas L-BFGS globalisé ?\n",
        "    use_lbfgs = (update is manifold_muon_general)\n",
        "\n",
        "    steps = epochs * len(train_loader)\n",
        "    step = 0\n",
        "\n",
        "    # ----- Projection initiale des paramètres manifold -----\n",
        "    if optimizer is None:\n",
        "        with torch.no_grad():\n",
        "            for p in model.parameters():\n",
        "                # Paramètres 2D (Linear) : on peut les mettre sur Stiefel\n",
        "                if p.ndim == 2:\n",
        "                    if use_lbfgs:\n",
        "                        opt_p = ManifoldLBFGS(eta=initial_lr, history=10)\n",
        "                        OPTS[p] = opt_p\n",
        "                        p.data = manifold_muon_general(\n",
        "                            p.data,\n",
        "                            torch.zeros_like(p.data),\n",
        "                            eta=0.0,\n",
        "                            opt=opt_p,\n",
        "                        )\n",
        "                    else:\n",
        "                        p.data = update(p.data, torch.zeros_like(p.data), eta=0.0)\n",
        "\n",
        "                # Paramètres 4D (conv) : on les aplati en matrice [out, in*kH*kW]\n",
        "                elif p.ndim == 4:\n",
        "                    shape = p.shape\n",
        "                    W = p.data.view(shape[0], -1)\n",
        "                    Z = torch.zeros_like(W)\n",
        "                    if use_lbfgs:\n",
        "                        opt_p = ManifoldLBFGS(eta=initial_lr, history=10)\n",
        "                        OPTS[p] = opt_p\n",
        "                        W_new = manifold_muon_general(W, Z, eta=0.0, opt=opt_p)\n",
        "                    else:\n",
        "                        W_new = update(W, Z, eta=0.0)\n",
        "                    p.data.copy_(W_new.view(shape))\n",
        "                # Le reste (bias, LayerNorm, pos_embed...) reste euclidien\n",
        "\n",
        "    epoch_losses = []\n",
        "    epoch_times = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        running_loss = 0.0\n",
        "        model.train()\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            lr = initial_lr * (1 - step / steps)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if optimizer is None:\n",
        "                    # --------- Cas Muon / hyperspherical / L-BFGS ---------\n",
        "                    for p in model.parameters():\n",
        "                        if p.grad is None:\n",
        "                            continue\n",
        "\n",
        "                        # Cas L-BFGS globalisé + param manifold\n",
        "                        if use_lbfgs and p in OPTS and p.ndim in (2, 4):\n",
        "                            opt_p = OPTS[p]\n",
        "\n",
        "                            if p.ndim == 2:\n",
        "                                W = p.data\n",
        "                                G = p.grad\n",
        "                            else:  # conv 4D\n",
        "                                shape = p.shape\n",
        "                                W = p.data.view(shape[0], -1)\n",
        "                                G = p.grad.view(shape[0], -1)\n",
        "\n",
        "                            # 1) On met à jour la courbure avec le gradient courant\n",
        "                            #    (opt.update utilisera l'info _pending du step précédent)\n",
        "                            opt_p.update(G)\n",
        "\n",
        "                            # 2) On fait un step quasi-Newton sur la variété\n",
        "                            W_new = manifold_muon_general(W, G, eta=lr, opt=opt_p)\n",
        "\n",
        "                            if p.ndim == 2:\n",
        "                                p.data.copy_(W_new)\n",
        "                            else:\n",
        "                                p.data.copy_(W_new.view(shape))\n",
        "\n",
        "                        else:\n",
        "                            # ----- Cas \"update\" stateless (manifold_muon / hyperspherical) -----\n",
        "                            if p.ndim == 2:\n",
        "                                p.data = update(p.data, p.grad, eta=lr)\n",
        "                            elif p.ndim == 4:\n",
        "                                shape = p.shape\n",
        "                                W = p.data.view(shape[0], -1)\n",
        "                                G = p.grad.view(shape[0], -1)\n",
        "                                W_new = update(W, G, eta=lr)\n",
        "                                p.data.copy_(W_new.view(shape))\n",
        "                            else:\n",
        "                                # Paramètres euclidiens (embeddings, LayerNorm, bias...)\n",
        "                                p.data = p.data - lr * p.grad\n",
        "                else:\n",
        "                    # --------- Cas AdamW ---------\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group[\"lr\"] = lr\n",
        "                    optimizer.step()\n",
        "\n",
        "            step += 1\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(\n",
        "                    f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], \"\n",
        "                    f\"Loss: {loss.item():.4f}\"\n",
        "                )\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_time = end_time - start_time\n",
        "        epoch_losses.append(epoch_loss)\n",
        "        epoch_times.append(epoch_time)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Time: {epoch_time:.4f} seconds\")\n",
        "\n",
        "    return model, epoch_losses, epoch_times\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# EVAL\n",
        "# -------------------------------------------------------------------\n",
        "def eval(model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        accs = []\n",
        "        for dataloader in [test_loader, train_loader]:\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in dataloader:\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            accs.append(100.0 * correct / total)\n",
        "\n",
        "    print(\n",
        "        f\"Accuracy of the network on the {len(test_loader.dataset)} test images: \"\n",
        "        f\"{accs[0]} %\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Accuracy of the network on the {len(train_loader.dataset)} train images: \"\n",
        "        f\"{accs[1]} %\"\n",
        "    )\n",
        "    return accs\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# WEIGHT STATS\n",
        "# -------------------------------------------------------------------\n",
        "def weight_stats(model):\n",
        "    singular_values = []\n",
        "    norms = []\n",
        "    for p in model.parameters():\n",
        "        norms.append(p.norm().item())\n",
        "        if p.ndim >= 2:\n",
        "            mat = p.view(p.size(0), -1)\n",
        "            u, s, v = torch.svd(mat)\n",
        "            singular_values.append(s)\n",
        "    return singular_values, norms\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# MAIN\n",
        "# -------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Train a model on CIFAR-10.\")\n",
        "    parser.add_argument(\"--model\", type=str, default=\"mlp\",\n",
        "                        choices=[\"mlp\", \"cnn\", \"vit\"],\n",
        "                        help=\"Model type to use.\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5,\n",
        "                        help=\"Number of epochs to train for.\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.1,\n",
        "                        help=\"Initial learning rate.\")\n",
        "    parser.add_argument(\n",
        "        \"--update\",\n",
        "        type=str,\n",
        "        default=\"manifold_muon_general\",\n",
        "        choices=[\"manifold_muon\", \"manifold_muon_general\",\n",
        "                 \"hyperspherical_descent\", \"adam\"],\n",
        "        help=\"Update rule to use.\",\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=42,\n",
        "                        help=\"Seed for the random number generator.\")\n",
        "    parser.add_argument(\"--wd\", type=float, default=0.0,\n",
        "                        help=\"Weight decay for AdamW.\")\n",
        "    args = parser.parse_args([])  # mets None si tu lances en CLI\n",
        "\n",
        "    # Determinism\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Dictionnaire des règles d'update\n",
        "    update_rules = {\n",
        "        \"manifold_muon\": manifold_muon,\n",
        "        \"manifold_muon_general\": manifold_muon_general,\n",
        "        \"hyperspherical_descent\": hyperspherical_descent,\n",
        "        \"adam\": AdamW,\n",
        "    }\n",
        "    update = update_rules[args.update]\n",
        "\n",
        "    print(f\"Training {args.model.upper()} with: {args.update}\")\n",
        "    print(\n",
        "        f\"Epochs: {args.epochs} --- LR: {args.lr}\"\n",
        "        + (f\" --- WD: {args.wd}\" if args.update == \"adam\" else \"\")\n",
        "    )\n",
        "\n",
        "    model, epoch_losses, epoch_times = train(\n",
        "        epochs=args.epochs,\n",
        "        initial_lr=args.lr,\n",
        "        update=update,\n",
        "        wd=args.wd,\n",
        "        model_type=args.model,\n",
        "    )\n",
        "    test_acc, train_acc = eval(model)\n",
        "    singular_values, norms = weight_stats(model)\n",
        "\n",
        "    results = {\n",
        "        \"model\": args.model,\n",
        "        \"epochs\": args.epochs,\n",
        "        \"lr\": args.lr,\n",
        "        \"seed\": args.seed,\n",
        "        \"wd\": args.wd,\n",
        "        \"update\": args.update,\n",
        "        \"epoch_losses\": epoch_losses,\n",
        "        \"epoch_times\": epoch_times,\n",
        "        \"test_acc\": test_acc,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"singular_values\": singular_values,\n",
        "        \"norms\": norms,\n",
        "    }\n",
        "\n",
        "    filename = (\n",
        "        f\"model-{args.model}-update-{args.update}-lr-{args.lr}\"\n",
        "        f\"-wd-{args.wd}-seed-{args.seed}.pkl\"\n",
        "    )\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    save_path = os.path.join(\"results\", filename)\n",
        "    print(f\"Saving results to {save_path}\")\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump(results, f)\n",
        "    print(f\"Results saved to {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"gpu\")\n",
        "\n",
        "%run \"/content/drive/MyDrive/Colab_Notebooks/EECS182_project/hyperspherical_descent.ipynb\"\n",
        "%run \"/content/drive/MyDrive/Colab_Notebooks/EECS182_project/LGFBS_global.ipynb\"\n",
        "# after this, the functions defined inside those notebooks are available in the current notebook\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avBi96pLrtIR",
        "outputId": "a9b489f1-11dc-43e3-d662-8d26be274a92"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}